{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"Arxiv\"]\n",
    "collection = db[\"Arxiv Papers\"]\n",
    "summary = [[doc[\"summary\"]] for doc in collection.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_summary = copy.deepcopy(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lecture note on optimization for machine learn derive from a course at princeton university and tutorial give in mlss buenos aire as well a simon foundation berkeley']\n"
     ]
    }
   ],
   "source": [
    "print(original_summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(summaries):\n",
    "    summaries[0] = summaries[0].lower()\n",
    "    summaries[0] = summaries[0].translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(summaries[0])\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tags\n",
    "    ]\n",
    "    return [\" \".join(lemmatized_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(summary)):\n",
    "    summary[i] = preprocessing(summary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(summary))\n",
    "train_data = summary[:train_size]\n",
    "test_data = summary[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fixed = [train_data[index][0] for index in range(len(train_data))]\n",
    "summary_full = [summary[index][0] for index in range(len(summary))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mat = tfidf_vectorizer.fit_transform(train_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30554, 67363)\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(tfidf_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tfidf = tfidf_vectorizer.transform([test_data[-3][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(new_tfidf, tfidf_mat)\n",
    "most_similar = similarities.argmax()\n",
    "score = similarities.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16988\n",
      "eeg signal be usually simple to obtain but expensive to label although supervised learning have be widely use in the field of eeg signal analysis it generalization performance be limit by the amount of annotate data selfsupervised learn ssl a a popular learning paradigm in computer vision cv and natural language processing nlp can employ unlabeled data to make up for the data shortage of supervised learning in this paper we propose a selfsupervised contrastive learn method of eeg signal for sleep stage classification during the training process we set up a pretext task for the network in order to match the right transformation pair generate from eeg signal in this way the network improve the representation ability by learn the general feature of eeg signal the robustness of the network also get improve in deal with diverse data that be extract constant feature from change data in detail the network performance depend on the choice of transformation and the amount of unlabeled data use in the training process of selfsupervised learn empirical evaluation on the sleepedf dataset demonstrate the competitive performance of our method on sleep staging 8816 accuracy and 8196 f1 score and verify the effectiveness of ssl strategy for eeg signal analysis in limited label data regime all code be provide publicly online\n",
      "federate learn fl be a distributed learning paradigm that enable a large number of device to collaboratively learn a model without share their raw data despite it practical efficiency and effectiveness the iterative ondevice learn process incur a considerable cost in term of learning time and energy consumption which depend crucially on the number of selected client and the number of local iteration in each training round in this paper we analyze how to design adaptive fl that optimally choose these essential control variable to minimize the total cost while ensure convergence theoretically we analytically establish the relationship between the total cost and the control variables with the convergence upper bound to efficiently solve the cost minimization problem we develop a lowcost samplingbased algorithm to learn the convergence relate unknown parameter we derive important solution property that effectively identify the design principles for different metric preference practically we evaluate our theoretical result both in a simulated environment and on a hardware prototype experimental evidence verifies our derived property and demonstrate that our propose solution achieve nearoptimal performance for various datasets different machine learning model and heterogeneous system setting\n",
      "0.9057189777018884\n"
     ]
    }
   ],
   "source": [
    "print(most_similar)\n",
    "print(test_data[-1][0])\n",
    "print(train_data[most_similar][0])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can input multiple things.\n",
    "1 - doi of the article they want to find (from doi we extract summary, preprocess then serves as input to vector) -- figure out how to query this later\n",
    "2 - summary of article (preprocess summary then find most similar article)\n",
    "3 - title if in db (use corresponding summary, preprocess then pass through model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user inputs the doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://export.arxiv.org/api/query?search_query=all:Machine%20Learning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user inputs summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = test_data[-3][0] # Already preprocessed, but simply pass through function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user inputs title:\n",
    "    - if title present in db get summary, preprocess then pass through model\n",
    "    - if title not present in db then preprocess title and pass through model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mat = tfidf_vectorizer.fit_transform(summary_full)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the user input to pass through model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tfidf = tfidf_vectorizer.transform([test_data[-3][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(new_tfidf, tfidf_mat)\n",
    "most_similar = similarities.argmax()\n",
    "score = similarities.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02331928 0.07301803 0.01945461 ... 1.         0.08941017 0.04466947]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33946\n"
     ]
    }
   ],
   "source": [
    "print(similarities[0].argmax())\n",
    "similarities_np = np.array(similarities[0])\n",
    "highest = np.argpartition(similarities_np, -5)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33946\n",
      "eeg signal be usually simple to obtain but expensive to label although supervised learning have be widely use in the field of eeg signal analysis it generalization performance be limit by the amount of annotate data selfsupervised learn ssl a a popular learning paradigm in computer vision cv and natural language processing nlp can employ unlabeled data to make up for the data shortage of supervised learning in this paper we propose a selfsupervised contrastive learn method of eeg signal for sleep stage classification during the training process we set up a pretext task for the network in order to match the right transformation pair generate from eeg signal in this way the network improve the representation ability by learn the general feature of eeg signal the robustness of the network also get improve in deal with diverse data that be extract constant feature from change data in detail the network performance depend on the choice of transformation and the amount of unlabeled data use in the training process of selfsupervised learn empirical evaluation on the sleepedf dataset demonstrate the competitive performance of our method on sleep staging 8816 accuracy and 8196 f1 score and verify the effectiveness of ssl strategy for eeg signal analysis in limited label data regime all code be provide publicly online\n",
      "federate learn fl be a distributed learning paradigm that enable a large number of mobile device to collaboratively learn a model under the coordination of a central server without share their raw data despite it practical efficiency and effectiveness the iterative ondevice learn process eg local computation and global communication with the server incur a considerable cost in term of learning time and energy consumption which depend crucially on the number of selected client and the number of local iteration in each training round in this paper we analyze how to design adaptive fl in mobile edge network that optimally choose these essential control variable to minimize the total cost while ensure convergence we establish the analytical relationship between the total cost and the control variables with the convergence upper bound to efficiently solve the cost minimization problem we develop a lowcost samplingbased algorithm to learn the convergence relate unknown parameter we derive important solution property that effectively identify the design principles for different optimization metric practically we evaluate our theoretical result both in a simulated environment and on a hardware prototype experimental evidence verifies our derived property and demonstrate that our propose solution achieve nearoptimal performance for different optimization metric for various datasets and heterogeneous system and statistical setting\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(most_similar)\n",
    "print(test_data[-1][0])\n",
    "print(summary_full[most_similar])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to return the title for the objects with the 5 highest scores (this can be changed by the user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.39460000e+04 1.00000000e+00]\n",
      " [1.69880000e+04 9.05447436e-01]\n",
      " [1.30200000e+04 3.13338517e-01]\n",
      " [1.27030000e+04 2.94950493e-01]\n",
      " [2.19550000e+04 2.91983855e-01]]\n"
     ]
    }
   ],
   "source": [
    "clean_arr = np.array([[high, similarities_np[high]] for high in highest])\n",
    "clean_arr = clean_arr[clean_arr[:,1].argsort()][::-1]\n",
    "print(clean_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['federate learn fl can be use in mobile edge network to train machine learn model in a distributed manner recently fl have be interpret within a modelagnostic metalearning maml framework which bring fl significant advantage in fast adaptation and convergence over heterogeneous datasets however exist research simply combine maml and fl without explicitly address how much benefit maml bring to fl and how to maximize such benefit over mobile edge network in this paper we quantify the benefit from two aspect optimize fl hyperparameters ie sample data size and the number of communication round and resource allocation ie transmit power in mobile edge network specifically we formulate the mamlbased fl design a an overall learning time minimization problem under the constraint of model accuracy and energy consumption facilitate by the convergence analysis of mamlbased fl we decompose the formulate problem and then solve it use analytical solution and the coordinate descent method with the obtain fl hyperparameters and resource allocation we design a mamlbased fl algorithm call automate federate learn autofl that be able to conduct fast adaptation and convergence extensive experimental result verify that autofl outperform other benchmark algorithm regard the learning time and convergence performance'\n",
      " 'federate learn fl refers to the learning paradigm that train machine learning model directly in the decentralized system consist of smart edge device without transmit the raw data which avoid the heavy communication cost and privacy concern give the typical heterogeneous data distribution in such situation the popular fl algorithm emphfederated average fedavg suffers from weight divergence and thus can not achieve a competitive performance for the global model denote a the emphinitial performance in fl compare to centralize method in this paper we propose the local continual training strategy to address this problem importance weight be evaluate on a small proxy dataset on the central server and then use to constrain the local training with this additional term we alleviate the weight divergence and continually integrate the knowledge on different local client into the global model which ensure a good generalization ability experiment on various fl setting demonstrate that our method significantly improve the initial performance of federated model with few extra communication cost'\n",
      " 'federate learn fl be a distributed learning paradigm that enable a large number of mobile device to collaboratively learn a model under the coordination of a central server without share their raw data despite it practical efficiency and effectiveness the iterative ondevice learn process eg local computation and global communication with the server incur a considerable cost in term of learning time and energy consumption which depend crucially on the number of selected client and the number of local iteration in each training round in this paper we analyze how to design adaptive fl in mobile edge network that optimally choose these essential control variable to minimize the total cost while ensure convergence we establish the analytical relationship between the total cost and the control variables with the convergence upper bound to efficiently solve the cost minimization problem we develop a lowcost samplingbased algorithm to learn the convergence relate unknown parameter we derive important solution property that effectively identify the design principles for different optimization metric practically we evaluate our theoretical result both in a simulated environment and on a hardware prototype experimental evidence verifies our derived property and demonstrate that our propose solution achieve nearoptimal performance for different optimization metric for various datasets and heterogeneous system and statistical setting'\n",
      " 'federate learn fl have open the opportunity for collaboratively train machine learn model on heterogeneous mobile or edge device while keep local data privatewith an increase in it adoption a grow concern be relate to it economic and environmental cost a be also the case for other machine learn techniquesunfortunately little work have be do to optimize it energy consumption or emission of carbon dioxide or equivalent a energy minimization be usually leave a a secondary objectivein this paper we investigate the problem of minimize the energy consumption of fl training on heterogeneous device by control the workload distributionwe model this a the minimal cost fl schedule problem a total cost minimization problem with identical independent and atomic task that have to be assign to heterogeneous resource with arbitrary cost functionswe propose a pseudopolynomial optimal solution to the problem base on the previously unexplored multiplechoice minimumcost maximal knapsack pack problemwe also provide four algorithm for scenario where cost function be monotonically increase and follow the same behaviorthese solution be likewise applicable on the minimization of other kind of cost and in other onedimensional data partition problem'\n",
      " 'federate learn fl be a distributed learning paradigm that enable a large number of device to collaboratively learn a model without share their raw data despite it practical efficiency and effectiveness the iterative ondevice learn process incur a considerable cost in term of learning time and energy consumption which depend crucially on the number of selected client and the number of local iteration in each training round in this paper we analyze how to design adaptive fl that optimally choose these essential control variable to minimize the total cost while ensure convergence theoretically we analytically establish the relationship between the total cost and the control variables with the convergence upper bound to efficiently solve the cost minimization problem we develop a lowcost samplingbased algorithm to learn the convergence relate unknown parameter we derive important solution property that effectively identify the design principles for different metric preference practically we evaluate our theoretical result both in a simulated environment and on a hardware prototype experimental evidence verifies our derived property and demonstrate that our propose solution achieve nearoptimal performance for various datasets different machine learning model and heterogeneous system setting']\n"
     ]
    }
   ],
   "source": [
    "print(np.array(summary_full)[highest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"Arxiv\"]\n",
    "collection = db[\"Arxiv Papers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lecture note on optimization for machine learn derive from a course at princeton university and tutorial give in mlss buenos aire as well a simon foundation berkeley']\n"
     ]
    }
   ],
   "source": [
    "print(original_summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('679de45665551d082bb28633'), 'doi': None, 'title': 'lecture notes: optimization for machine learning', 'summary': 'lecture notes on optimization for machine learning, derived from a course at princeton university and tutorials given in mlss, buenos aires, as well as simons foundation, berkeley.', 'authors': ['elad hazan'], 'summary_embedded': [[-0.03295045346021652, -0.049884021282196045, 0.02169964089989662, -0.042217276990413666, 0.021081535145640373, -0.007480995263904333, 0.05210074409842491, -0.014372649602591991, -0.062026433646678925, -0.00013597724318969995, -0.042934391647577286, 0.08189061284065247, 0.020059265196323395, -0.04340370371937752, -0.03441626951098442, 0.06136669963598251, 0.010546771809458733, 0.01702974922955036, -0.031569842249155045, -0.09214671701192856, 0.0743427574634552, 0.007017235271632671, 0.0003597683389671147, -0.014236473478376865, 0.04310692101716995, -0.04300112649798393, 0.02624274604022503, 0.05065550655126572, 0.019436949864029884, -0.021306689828634262, -0.001261350349523127, -0.025703730061650276, 0.06330431252717972, -0.01376259233802557, 0.0013045610394328833, 0.04754488542675972, 0.02118098735809326, -0.025389114394783974, 0.026536159217357635, 0.06422845274209976, -0.03925056755542755, 0.014709098264575005, 0.008481272496283054, 0.0072290170937776566, 0.037969306111335754, 0.07478807866573334, -0.005060933530330658, -0.06094628572463989, 0.006180359050631523, -0.01275479793548584, -0.06481603533029556, -0.012588218785822392, -0.09318675100803375, 0.02157309092581272, -0.08792177587747574, 0.036609210073947906, 0.058979641646146774, 0.016565322875976562, 0.021761640906333923, 0.024680061265826225, 0.006859874818474054, -0.1700296849012375, -0.04603607952594757, 0.024390649050474167, 0.0036350057926028967, 0.009851411916315556, 0.023953109979629517, 0.07432637363672256, -0.051972899585962296, 0.008245524019002914, -0.035721804946660995, 0.050593722611665726, -0.04915665090084076, 0.05341591313481331, 0.04897698387503624, -0.009773479774594307, 0.08202966302633286, 0.004883543122559786, 0.005937967915087938, 0.002033904194831848, -1.4119518709776457e-06, 0.014596309512853622, -0.042318008840084076, -0.021685251966118813, 0.03463234752416611, -0.07151336967945099, -0.10402805358171463, -0.023145388811826706, 0.09687264263629913, -0.009007980115711689, 0.01869751699268818, 0.016977079212665558, -0.02208731323480606, -0.011609404347836971, -0.029997147619724274, -0.008762174285948277, 0.005627663806080818, -0.057590924203395844, 0.008774891495704651, 0.1035645380616188, -0.06660432368516922, -0.012916194275021553, 0.034404825419187546, 0.0053526852279901505, 0.027985883876681328, -0.07214752584695816, -0.013055916875600815, 0.07114831358194351, 0.0695914477109909, -0.08440843224525452, 0.05069420486688614, 0.000507624470628798, 0.0005081753479316831, 0.00668162340298295, -0.02363424189388752, -0.02794489823281765, 0.11917338520288467, -0.007159008178859949, 0.03336271271109581, 0.03129372373223305, -0.03320550546050072, -0.0500006265938282, -0.040827419608831406, 0.008734938688576221, -0.01335352286696434, 0.0014841776574030519, -0.038578394800424576, 9.594187445094771e-34, -0.03933071345090866, 0.03552982956171036, 0.013197891414165497, -0.06965713948011398, 0.013028878718614578, -0.03011387400329113, 0.059079643338918686, -0.027667447924613953, 0.007592708803713322, 0.02237432636320591, -0.02214249223470688, -0.021458476781845093, -0.05937434360384941, 0.06684945523738861, 0.05072419345378876, -0.057268209755420685, 0.011170709505677223, 0.041566152125597, -0.030403494834899902, -0.08287829905748367, 0.058316610753536224, -0.020050445571541786, 0.024537255987524986, -0.017475120723247528, -0.011514782905578613, 0.0016696476377546787, 0.07873762398958206, -0.06473933160305023, -0.005516058765351772, 0.027975475415587425, -0.05960407108068466, 0.03868420049548149, -0.020328421145677567, 0.08320367336273193, 0.06591281294822693, 0.032930511981248856, -0.02030463144183159, 0.029480906203389168, 0.005595298018306494, 0.034711744636297226, -0.036568015813827515, 0.05901229381561279, 0.07587901502847672, -0.11122752726078033, -0.042131852358579636, -0.049729593098163605, 0.01770075596868992, 0.04815901443362236, 0.1172623410820961, -0.12385858595371246, -0.05995192378759384, -0.054375164210796356, -0.012153787538409233, -0.014841757714748383, 0.0587649904191494, 0.026932109147310257, -0.00894878339022398, -0.004598027560859919, -0.04823623597621918, 0.017372705042362213, -0.0004622628621291369, 0.01171098742634058, 0.07540366798639297, -0.04624844714999199, -0.03769843280315399, 0.0035308857914060354, 0.012912141159176826, 0.0093715600669384, 0.07150998711585999, -0.049545250833034515, -0.008587989024817944, 0.04465045779943466, 0.08344793319702148, -0.08482931554317474, 0.03876940533518791, 0.012124180793762207, 0.04560435563325882, 0.04274500906467438, -0.0824059545993805, -0.013171928003430367, -0.07895200699567795, 0.07295354455709457, -0.005100021604448557, -0.06804526597261429, -0.008988769724965096, -0.057606738060712814, 0.045807793736457825, -0.023020610213279724, -0.1364881545305252, -0.020515115931630135, -0.1179465726017952, 0.04757661372423172, -0.02437261864542961, 0.0435253269970417, -0.04489028826355934, -2.3937577779022977e-33, -0.0604732483625412, -0.01767072081565857, 0.010485377162694931, 0.01889895088970661, -7.433862629113719e-05, 0.03890242055058479, -0.08023323863744736, 0.012849662452936172, -0.013003863394260406, -0.031079547479748726, -0.09374416619539261, 0.010721991769969463, 0.026606813073158264, -0.029781080782413483, -0.041403260082006454, 0.026715656742453575, 0.024738121777772903, -0.039217665791511536, 0.004198760725557804, -0.017055131494998932, 0.0035008813720196486, 0.11703240871429443, -0.02756417915225029, -0.028112659230828285, 0.026167074218392372, -0.07868814468383789, -0.01753246784210205, 0.10087431222200394, -0.04353925585746765, -0.043633922934532166, -0.08227390795946121, 0.038752615451812744, -0.09116073697805405, 0.03354552388191223, -0.0567164346575737, 0.040458206087350845, -0.005744392983615398, 0.041814133524894714, 0.023825189098715782, 0.1683085411787033, 0.044951945543289185, 0.016814937815070152, -0.04113563522696495, -0.07493435591459274, 0.04546566307544708, -0.055131521075963974, -0.014910297468304634, -0.07967980206012726, -0.02129962295293808, -0.01519908756017685, -0.009586485102772713, -0.01164007093757391, -0.05952652171254158, -0.003770293667912483, -0.0682671070098877, -0.01538331899791956, 0.004792504012584686, -0.07360108941793442, 0.08156035095453262, 0.0060372548177838326, -0.060606181621551514, 0.014541348442435265, -0.1163039430975914, 0.11707036942243576, -0.007519007660448551, -0.034058671444654465, 0.014458313584327698, -0.024183234199881554, 0.022243740037083626, 0.044578731060028076, -0.08437126129865646, 0.06609123200178146, 0.05039291828870773, 0.035709917545318604, -0.054753147065639496, -0.009499114006757736, -0.014864378608763218, 0.0026051390450447798, -0.039565566927194595, -0.0305524580180645, 0.09201844781637192, 0.014882531948387623, -0.032740939408540726, 0.1362716257572174, 0.0837956890463829, 0.05018028989434242, 0.09954433143138885, 0.0673496425151825, -0.002815431449562311, -0.060816120356321335, -0.023160463199019432, 0.026176629588007927, 0.006484161596745253, -0.011484570801258087, -0.03841744363307953, -3.087059852191487e-08, -0.06506183743476868, 0.03454766049981117, 0.03300962224602699, -0.00829736702144146, 0.1054944396018982, 0.0448053814470768, 0.004435892682522535, 0.06362835317850113, -0.07136084884405136, -0.004559190012514591, 0.07137235254049301, 0.010849619284272194, -0.008084983564913273, 0.006959408055990934, -0.09613815695047379, 0.04188825190067291, -0.05910789966583252, 0.021497422829270363, -0.005512547679245472, -0.011885276064276695, 0.018969586119055748, 0.03405940532684326, 0.060911957174539566, -0.033277761191129684, 0.10117220878601074, -0.09184131026268005, -0.01679135486483574, 0.002801550319418311, 0.06025930866599083, 0.010812091641128063, -0.12943778932094574, 0.054181620478630066, 0.023972628638148308, -0.021812496706843376, 0.08293873071670532, 0.05353064090013504, 0.04361940920352936, -0.057613275945186615, -0.07725078612565994, 0.02271251752972603, 0.02112276293337345, 0.03871302679181099, -0.07891107350587845, -0.028398919850587845, 0.10031811892986298, 0.04386863857507706, 0.020932871848344803, -0.0013984957477077842, -0.05838482454419136, 0.027122557163238525, 0.09456728398799896, -0.021879227831959724, 0.03121858648955822, 0.060958124697208405, 0.06777901202440262, -0.026334641501307487, 0.04503301531076431, -0.06097320467233658, 0.03714808076620102, 0.038389191031455994, 0.01468395534902811, 0.11038163304328918, -0.11812642216682434, 0.011213689111173153], [-0.11883839219808578, 0.048298779875040054, -0.00254812091588974, -0.011011196300387383, 0.05195077881217003, 0.010291741229593754, 0.11543327569961548, 0.0007007556268945336, -0.08592539280653, -0.07065404206514359, 0.0013318165438249707, -0.035472240298986435, 0.018434086814522743, -0.006737183779478073, 0.024402951821684837, -0.02950318343937397, -0.05813842639327049, -0.05043955519795418, -0.020765451714396477, 0.029036005958914757, -0.06367599219083786, 0.02402995526790619, 0.026243330910801888, -0.0060373153537511826, -0.011076582595705986, -0.0014007423305884004, -0.018619878217577934, 0.032770104706287384, 0.0028860990423709154, -0.05694401264190674, -0.043941743671894073, 0.025414088740944862, 0.08790943771600723, -0.024991221725940704, -0.0366831049323082, 0.0062413583509624004, -0.06646788865327835, -0.06714440882205963, 0.020564237609505653, 0.04238879308104515, 0.02188025787472725, -0.042882420122623444, -0.034376949071884155, 0.06146686151623726, 0.06563734263181686, -0.0785202905535698, 0.02948697842657566, 0.010798228904604912, 0.06332410126924515, -0.045084740966558456, -0.018234042450785637, -0.0277210995554924, -0.00367385963909328, -0.03659450262784958, 0.05425015464425087, -0.020856546238064766, 0.015034941956400871, -0.06009512022137642, 0.016393957659602165, -0.03323860839009285, 0.0175035260617733, -0.0005952200153842568, -0.16348369419574738, 0.08492085337638855, -0.07583830505609512, 0.01610984094440937, 0.0483829639852047, -0.007598108146339655, -0.02498549036681652, 0.05949742719531059, 0.06588991731405258, -0.03513740375638008, 0.0008843130781315267, -0.11567974090576172, 0.04939030855894089, 0.03360462933778763, 0.05515419691801071, 0.02638370916247368, 0.0536944679915905, 0.038932450115680695, 0.0004394061106722802, 0.018060490489006042, -0.09288249164819717, -0.0040739551186561584, -0.0008235204149968922, -0.04883108660578728, -0.00667747063562274, -0.023541683331131935, -0.03813307359814644, 0.05245164409279823, -0.04249381646513939, -0.055899810045957565, 0.08681576699018478, -0.048961788415908813, -0.0833967998623848, -0.04576360806822777, 0.029042232781648636, 0.03465777263045311, -0.08649181574583054, 0.4062184989452362, 0.0359494648873806, 0.01869719661772251, 0.09797830879688263, -0.007865219376981258, 0.023714112117886543, -0.057565100491046906, -0.061099737882614136, -0.006620414089411497, 0.007060033269226551, 0.021669860929250717, -0.024405168369412422, -0.03351461887359619, 0.00025012929108925164, 0.03170771896839142, 0.04407160356640816, 0.09463231265544891, -0.03557995334267616, -0.004534303676337004, 0.04371495172381401, 0.00020505173597484827, -0.0028586871922016144, -0.024883940815925598, 0.0037607112899422646, 0.01404118724167347, 0.07781582325696945, -0.13231441378593445, 0.006876415107399225, -7.220122870988593e-33, 0.007334593683481216, 0.002726075705140829, 0.012147520668804646, -0.0024403163697570562, 0.027932537719607353, 0.03927067667245865, 0.0037439223378896713, -0.04643525555729866, -0.014492489397525787, 0.053601961582899094, 0.006590616423636675, 0.03664793819189072, -0.023135574534535408, 0.032753776758909225, 0.07811087369918823, 0.009627451188862324, 0.007964077405631542, 0.0028743220027536154, -0.0018807605374604464, 0.004691665060818195, -0.012402208521962166, -0.0008042618865147233, -0.023038627579808235, 0.04297289997339249, -0.02825993113219738, -0.06694649904966354, 0.03853899985551834, -0.07085712254047394, 0.020109394565224648, 0.0014602886512875557, 0.0014638800639659166, 0.04991228133440018, -0.025945616886019707, 0.0008223687182180583, -0.03757277876138687, -0.028740614652633667, 0.033375080674886703, -0.07462834566831589, -0.035983938723802567, 0.02568083256483078, -0.050139084458351135, 0.010837240144610405, -0.04243797808885574, -0.002668630564585328, -0.004916310776025057, 0.1664792001247406, -0.0011540575651451945, -0.004960628692060709, -0.06482204049825668, 0.06976211816072464, -0.002818108070641756, -0.0213251281529665, -0.11613688617944717, 0.043338727205991745, -0.003350921906530857, -0.020106516778469086, 0.016553929075598717, -0.04397117346525192, 0.020619388669729233, -0.009090038016438484, 0.009713687933981419, 0.03939149156212807, -0.012487691827118397, 0.009350251406431198, -0.08647783100605011, -0.048517726361751556, 0.024477781727910042, -0.00849495641887188, 0.023063573986291885, -0.012638235464692116, -0.05101002752780914, 0.03675990551710129, 0.037717387080192566, 0.030916042625904083, -0.028798460960388184, -0.019268766045570374, -0.019831756129860878, 0.03583518788218498, 0.08063049614429474, 0.006497282534837723, 0.035455357283353806, -0.04195889085531235, 0.006693840492516756, -0.024078911170363426, 0.09502361714839935, 0.05463499203324318, 0.004220996517688036, -0.05180741101503372, 0.010215237736701965, -0.04109857603907585, -0.03574549779295921, 0.06131811439990997, -0.0030944079626351595, 0.08796171844005585, 0.006000801920890808, 4.4925641867161134e-33, -0.07716734707355499, 0.018993034958839417, -0.03573815897107124, 0.088797926902771, -0.017555110156536102, -0.0027626079972833395, 0.037273991852998734, 0.09013667702674866, -0.09250438958406448, 0.06802995502948761, 0.022390158846974373, -0.04508966580033302, 0.03087892010807991, 0.044495195150375366, -0.005799545906484127, 0.0352335199713707, 0.06968846172094345, -0.004063496366143227, -0.02815510705113411, -0.03572942689061165, -0.03050716035068035, -0.03237845003604889, -0.002499824156984687, 0.03492945432662964, -0.04148074612021446, 0.03020525723695755, 0.04858914390206337, 0.06329885125160217, -0.021693119779229164, 0.036800459027290344, 0.03896573185920715, -0.023581378161907196, -0.05063263699412346, -0.05820288881659508, 0.04826255515217781, 0.08404393494129181, 0.03678106889128685, -0.0007769166259095073, 0.02484821155667305, -0.05051743984222412, 0.03966904804110527, -0.010082799009978771, 0.002244452014565468, 0.1169772818684578, -0.021961238235235214, -0.005805978551506996, -0.04809289798140526, 0.0037888623774051666, 0.03517270088195801, 0.07729730755090714, -0.09319712221622467, -0.011992985382676125, -0.021968122571706772, 0.04129423201084137, -0.022958217188715935, 0.004160349257290363, -0.043218761682510376, 0.0702131912112236, -0.01905953884124756, 0.0004752397071570158, 0.005480794236063957, 0.026761461049318314, -0.03361278399825096, 0.013468591496348381, -0.022746771574020386, 0.038738951086997986, -0.024523235857486725, -0.03632810711860657, -0.0017924690619111061, -0.05256984382867813, 0.006689414847642183, -0.0258465725928545, -0.13483542203903198, 0.0011393746826797724, -0.047169312834739685, -0.05347489193081856, -0.018427174538373947, -0.007304128259420395, -0.009657058864831924, -0.03772619739174843, -0.03399992361664772, 0.018417349085211754, -0.00800311379134655, -0.005512378644198179, -0.03353201225399971, -0.020180633291602135, 0.02166580781340599, 0.010758270509541035, -0.057474713772535324, 0.019696738570928574, -0.007240883074700832, 0.023037102073431015, 0.12023400515317917, 0.003241879865527153, 0.010149956680834293, -1.3403665732880654e-08, -0.046724583953619, 0.040620625019073486, -0.05561637505888939, -0.0018854326335713267, 0.056323908269405365, 0.049638886004686356, -0.041541535407304764, 0.03250390291213989, 0.025749219581484795, -0.018781006336212158, 0.06920824944972992, 0.025987928733229637, -0.027823321521282196, 0.05757514759898186, 0.09128089994192123, -0.015325825661420822, -0.10472099483013153, -0.027585946023464203, -0.016222761943936348, -0.03539936989545822, -0.010461247526109219, -0.013999276794493198, -0.00029417272889986634, -0.0836298018693924, 0.007932260632514954, 0.006960022263228893, -0.04422970861196518, 0.07475830614566803, 0.07440946996212006, -0.04058080539107323, -0.0018266943516209722, 0.019849997013807297, 0.014382107183337212, 0.020585326477885246, 0.022133776918053627, -0.0643705353140831, -0.0636984333395958, 0.016139158979058266, 0.009907360188663006, -0.005559565499424934, -0.054673098027706146, -0.02331147901713848, 0.07046927511692047, 0.00646799523383379, -0.04770008847117424, -0.0036471523344516754, 0.007837554439902306, -0.004974613431841135, -0.01241864450275898, -0.07781211286783218, -0.000940884870942682, -0.008002525195479393, 0.006034218706190586, 0.08434932678937912, 0.10730371624231339, 0.01142775360494852, 0.013366621918976307, -0.012747270986437798, 0.061454225331544876, 0.035641323775053024, 0.15874585509300232, 0.12640950083732605, 0.04654904827475548, -0.015717264264822006]], 'title_embedded': [-0.05595744028687477, -0.012472765520215034, 0.022684792056679726, -0.030525237321853638, -0.011910936795175076, -0.019338445737957954, 0.08461597561836243, -0.04067140445113182, -0.08405303210020065, 0.005618504248559475, -0.035029295831918716, 0.11640265583992004, 0.007004272658377886, -0.058780327439308167, -0.06454074382781982, 0.05499634891748428, 0.029719702899456024, 0.026014011353254318, -0.020523877814412117, -0.07676468789577484, 0.0390569232404232, 0.05223312973976135, 0.007156766951084137, -0.0219500120729208, 0.07270482182502747, -0.0343126505613327, 0.04256212338805199, 0.04374969005584717, 0.04060357064008713, -0.03109080344438553, -0.002926871180534363, 0.009855837561190128, 0.0300595723092556, 0.011544429697096348, -0.032020702958106995, 0.04323451966047287, 0.010410604998469353, 0.000980275683104992, 0.013419786468148232, 0.043306171894073486, -0.03042970784008503, 0.063191257417202, -0.020135696977376938, 0.014746587723493576, 0.08507637679576874, 0.07145337015390396, -0.010567129589617252, -0.03596553951501846, 0.021624799817800522, -0.027529684826731682, -0.05408516898751259, -0.0008736795280128717, -0.10945546627044678, 0.0019857895094901323, -0.07265520095825195, 0.019666127860546112, 0.0581386424601078, -0.0175437293946743, 0.017179789021611214, 0.0049928040243685246, 0.060593705624341965, -0.13799890875816345, -0.05817904695868492, 0.04059293866157532, 0.03901531547307968, -0.018264565616846085, 0.0471244640648365, 0.0670211911201477, -0.06466645002365112, 0.0765049085021019, -0.03264622390270233, 0.0736294612288475, -0.05621049925684929, 0.019265659153461456, 0.003193196374922991, -0.02577332779765129, 0.06053665280342102, 0.03489943966269493, 0.005672000348567963, -0.0177967119961977, 0.02995333820581436, -0.024770250543951988, 0.003778348909690976, -0.02850564382970333, 0.039389658719301224, -0.089157335460186, -0.0764557346701622, -0.04682842269539833, 0.05554932355880737, -0.017080608755350113, 0.01680896244943142, 0.022302908822894096, -0.049295783042907715, -0.012685795314610004, -0.030965222045779228, -0.012266128323972225, 0.015781739726662636, -0.07787743210792542, 0.062359441071748734, 0.11557091772556305, -0.03877435252070427, -0.028599726036190987, 0.09096089750528336, -0.011685623787343502, -0.008262613788247108, -0.06073621287941933, 0.008478628471493721, 0.06531984359025955, 0.11358683556318283, -0.08693281561136246, 0.04263046756386757, 0.018780376762151718, -0.019679175689816475, -0.002939390717074275, 0.03817848488688469, -0.06061283126473427, 0.08552446216344833, 0.007136705331504345, 0.0674607902765274, 0.0688585638999939, -0.04558582603931427, -0.05805281922221184, -0.010361180640757084, 0.022579267621040344, -0.017847279086709023, -0.0010886219097301364, -0.11109839379787445, -1.0732069650110592e-33, -0.017008844763040543, 0.01874271221458912, -0.022639138624072075, -0.05954151973128319, -0.0028221593238413334, 0.011233377270400524, 0.019086666405200958, -0.0670115277171135, 0.023431401699781418, 0.0050707110203802586, -0.046383846551179886, -0.04342115297913551, -0.028727203607559204, 0.056867267936468124, 0.04536288231611252, -0.036039598286151886, -0.010011887177824974, 0.10998892784118652, -0.027511317282915115, -0.08729003369808197, 0.0475408211350441, -0.0707000121474266, 0.01984894461929798, -0.01986813172698021, -0.039746981114149094, 5.4436823120340705e-05, 0.09333908557891846, -0.04869887977838516, 0.011119266971945763, 0.027215516194701195, -0.02079661190509796, 0.0069891768507659435, 0.009883586317300797, 0.027346251532435417, 0.05824046581983566, 0.04424325004220009, -0.013602089136838913, 0.024307502433657646, 0.028409382328391075, -0.01984909549355507, -0.02250915952026844, 0.05633637309074402, 0.09833794832229614, -0.13322098553180695, -0.024543168023228645, -0.016468945890665054, 0.036473795771598816, 0.057832807302474976, 0.08441568166017532, -0.10077407956123352, -0.08851804584264755, -0.06383861601352692, -0.03491342440247536, 0.018724244087934494, 0.0546945184469223, -0.0005082255811430514, 0.003157527418807149, -0.033999886363744736, -0.04459226876497269, 0.035980693995952606, -0.0049761817790567875, -0.007279014214873314, 0.051851700991392136, -0.0363883301615715, -0.042784109711647034, 0.006441543810069561, -0.030568553134799004, 0.04815466329455376, 0.035638101398944855, -0.04018440842628479, -0.00934617780148983, 0.0400664284825325, 0.03424481675028801, -0.09140174090862274, 0.037981197237968445, -0.000816693645901978, 0.03171016648411751, 0.019386131316423416, -0.11047224700450897, -0.021167593076825142, -0.06993290036916733, 0.023371146991848946, -0.02539917081594467, -0.07020486146211624, -0.05238280072808266, -0.08144430816173553, 0.03705334663391113, -0.04927555099129677, -0.08328827470541, 0.03067292831838131, -0.1457512378692627, 0.054204050451517105, -0.025930248200893402, 0.06611904501914978, -0.058945804834365845, 1.1618506955412053e-34, -0.05314520001411438, 0.036467622965574265, 0.024254146963357925, 0.043208491057157516, 0.00026819188497029245, 0.02238454855978489, -0.05683993175625801, 0.00858895480632782, -0.027603279799222946, -0.010843104682862759, -0.06746789067983627, 0.012323128990828991, 0.020325999706983566, -0.024298837408423424, -0.043369729071855545, 0.02660655789077282, 0.011584408581256866, -0.010328433476388454, 0.016927402466535568, -0.004701784811913967, -0.014804704114794731, 0.10438180714845657, -0.04978145658969879, 0.031024744734168053, 0.03689035773277283, -0.022839102894067764, -0.017545780166983604, 0.08209054917097092, -0.016021229326725006, -0.017520491033792496, -0.09085023403167725, 0.03412362188100815, -0.13255420327186584, 0.05301060155034065, -0.021218659356236458, 0.05759130418300629, 0.01751074381172657, 0.016260899603366852, 0.019228560850024223, 0.1738877296447754, 0.06838882714509964, 0.05405808612704277, -0.04926411435008049, -0.029642662033438683, 0.05430176109075546, -0.05770803987979889, 0.013971993699669838, -0.03064897656440735, 0.014367157593369484, -0.002761693438515067, -0.007090308703482151, 0.004830779042094946, 0.017416495829820633, -0.021126212552189827, -0.04345051944255829, 0.004118658602237701, -0.013579138554632664, -0.12665888667106628, 0.09205342829227448, 0.011599341407418251, -0.0991075336933136, -0.024165425449609756, -0.11119170486927032, 0.10808300226926804, -0.015716001391410828, -0.041207775473594666, 0.028279511258006096, 0.0025616309139877558, 0.06270305812358856, 0.02261030673980713, -0.020201748237013817, 0.08804169297218323, 0.037376947700977325, 0.01676829904317856, -0.05083099380135536, -0.012857966125011444, 0.005817916244268417, -0.025530844926834106, -0.06227091699838638, -0.022268066182732582, 0.07463973015546799, 0.035952579230070114, -0.02794571965932846, 0.10560762137174606, 0.026287587359547615, 0.04789828136563301, 0.0942792221903801, 0.03597169369459152, -0.007338234689086676, -0.045657265931367874, -0.044308483600616455, 0.04378826543688774, 0.0338754840195179, -0.015310690738260746, -0.0379791222512722, -1.3792890385389e-08, -0.05664154887199402, -0.02505103498697281, 0.028995594009757042, -0.04461057856678963, 0.07896433025598526, -0.006381047889590263, -0.02265394851565361, 0.0407041534781456, -0.06562352925539017, 0.014352716505527496, 0.05570882558822632, -0.023230206221342087, 0.0011720325564965606, 0.027190126478672028, -0.047831129282712936, 0.05032084882259369, -0.06518437713384628, 0.010757878422737122, -0.03628982976078987, -0.02717040292918682, 0.029622960835695267, 0.007270169444382191, 0.060758043080568314, 0.014807767234742641, 0.08110219240188599, -0.038419585675001144, 0.05880022048950195, 0.05654323846101761, 0.10848969966173172, 0.03956567496061325, -0.11910998821258545, 0.08788887411355972, -0.035915687680244446, -0.01557617075741291, 0.0730007141828537, 0.04799318686127663, 0.0426056943833828, -0.010948737151920795, -0.06371670216321945, 0.048280615359544754, -0.03407495841383934, 0.03580474108457565, -0.04596235230565071, -0.032730553299188614, 0.10722963511943817, 0.038386743515729904, -0.00607842905446887, -0.022554051131010056, -0.06312037259340286, 0.018123282119631767, 0.06590605527162552, -0.0039581418968737125, 0.03733821585774422, 0.0743601843714714, 0.012828021310269833, 0.026575248688459396, 0.012089606374502182, -0.04135368391871452, 0.012753776274621487, 0.04389248788356781, 0.013354512862861156, 0.08004460483789444, -0.14396801590919495, -0.014095546677708626]}\n"
     ]
    }
   ],
   "source": [
    "#summary_top_5 = np.array(original_summary)[highest]\n",
    "\n",
    "#print(summary_top_5[1][0])\n",
    "query = {\"summary\" : original_summary[0][0]}\n",
    "found = collection.find(query)\n",
    "for doc in found:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lecture note on optimization for machine learn derive from a course at princeton university and tutorial give in mlss buenos aire as well a simon foundation berkeley\n"
     ]
    }
   ],
   "source": [
    "print(original_summary[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we return to the user, namely, the title of the respective articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automated federated learning in mobile edge networks -- fast adaptation\n",
      "  and convergence \n",
      "\n",
      "continual local training for better initialization of federated models \n",
      "\n",
      "cost-effective federated learning in mobile edge networks \n",
      "\n",
      "scheduling algorithms for federated learning with minimal energy\n",
      "  consumption \n",
      "\n",
      "cost-effective federated learning design \n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_top_5 = np.array(original_summary)[highest]\n",
    "for summarized in summary_top_5:\n",
    "    query = {\"summary\" : summarized[0]}\n",
    "    found = collection.find(query)\n",
    "    for document in found:\n",
    "        print(document['title'], \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
