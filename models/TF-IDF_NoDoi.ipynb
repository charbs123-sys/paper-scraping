{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"Arxiv\"]\n",
    "collection = db[\"Arxiv Papers\"]\n",
    "summary = [[doc[\"summary\"]] for doc in collection.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(summaries):\n",
    "    summaries[0] = summaries[0].lower()\n",
    "    summaries[0] = summaries[0].translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(summaries[0])\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tags\n",
    "    ]\n",
    "    return [\" \".join(lemmatized_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(summary)):\n",
    "    summary[i] = preprocessing(summary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(summary))\n",
    "train_data = summary[:train_size]\n",
    "test_data = summary[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fixed = [train_data[index][0] for index in range(len(train_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mat = tfidf_vectorizer.fit_transform(train_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7920, 26999)\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(tfidf_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tfidf = tfidf_vectorizer.transform([test_data[-1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(new_tfidf, tfidf_mat)\n",
    "most_similar = similarities.argmax()\n",
    "score = similarities.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6193\n",
      "we present vbpimixtures an algorithm design to enhance the accuracy of phylogenetic posterior distribution particularly for treetopology and branchlength approximation despite the variational bayesian phylogenetic inference vbpi a leadingedge blackbox variational inference bbvi framework achieve remarkable approximation of these distribution the multimodality of the treetopology posterior present a formidable challenge to samplingbased learn technique such a bbvi advance deep learn methodology such a normalizing flow and graph neural network have be explore to refine the branchlength posterior approximation yet effort to ameliorate the posterior approximation over tree topology have be lack our novel vbpimixtures algorithm bridge this gap by harness the late breakthrough in mixture learning within the bbvi domain a a result vbpimixtures be capable of capture distribution over treetopologies that vbpi fail to model we deliver stateoftheart performance on difficult density estimation task across numerous real phylogenetic datasets\n",
      "metalearning be a family of method that use a set of interrelated task to learn a model that can quickly learn a new query task from a possibly small contextual dataset in this study we use a probabilistic framework to formalize what it mean for two task to be relate and reframe the metalearning problem into the problem of bayesian risk minimization brm in our formulation the brm optimal solution be give by the predictive distribution compute from the posterior distribution of the taskspecific latent variable condition on the contextual dataset and this justify the philosophy of neural process however the posterior distribution in neural process violate the way the posterior distribution change with the contextual dataset to address this problem we present a novel gaussian approximation for the posterior distribution that generalize the posterior of the linear gaussian model unlike that of the neural process our approximation of the posterior distribution converges to the maximum likelihood estimate with the same rate a the true posterior distribution we also demonstrate the competitiveness of our approach on benchmark datasets\n",
      "0.3472466930457542\n"
     ]
    }
   ],
   "source": [
    "print(most_similar)\n",
    "print(test_data[-1][0])\n",
    "print(train_data[most_similar][0])\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
